{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-tagging.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2IVMmkbG-ZS",
        "colab_type": "text"
      },
      "source": [
        "# Using NLTK tagging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkxiLcIoFpm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1e77136e-ad69-4bf9-bdd1-27485556c6cd"
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak9_MzsKGpJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d1e61418-dad0-459a-fb86-12daa151df95"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "javgK1VxF2Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"Clement and Mathieu are working at Apple.\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ObZfQf2F5Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize, pos_tag and do the chuncking\n",
        "\n",
        "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ELhSPiGAYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "26a138f0-2146-4230-976a-d6c7bc2d449e"
      },
      "source": [
        "iob_tagged = tree2conlltags(ne_tree)\n",
        "print(iob_tagged)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Clement', 'NN', 'B-GPE'), ('and', 'CC', 'O'), ('Mathieu', 'NNP', 'B-PERSON'), ('are', 'VBP', 'O'), ('working', 'VBG', 'O'), ('at', 'IN', 'O'), ('Apple', 'NNP', 'B-ORGANIZATION'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aX4oT3QG3O0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8f8ae24f-b710-473c-8b27-17a19181ca67"
      },
      "source": [
        "ne_tree = conlltags2tree(iob_tagged)\n",
        "print(ne_tree)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (GPE Clement/NN)\n",
            "  and/CC\n",
            "  (PERSON Mathieu/NNP)\n",
            "  are/VBP\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Apple/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTopCEooIo_D",
        "colab_type": "text"
      },
      "source": [
        "# Stanford Named Entity Recognizer (NER)\n",
        "\n",
        "Stanford's tagger uses the same algorithm. While it is written in Java and to use it you must\n",
        "download the JAR files to use it (you can find these files on the website), NLTK offers us a\n",
        "Python interface to access the tagger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq9eUGjDHifV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d0154641-398b-408d-bc60-79ddd8a71641"
      },
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "st = StanfordNERTagger('/content/english.muc.7class.distsim.crf.ser.gz', '/content/stanford-ner.jar', encoding='utf-8')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W0Rhbn7KKcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "7ba99cdd-f83a-4d50-84b6-7bdd99638bc3"
      },
      "source": [
        "st.tag('Baptiste Capdeville is studying at Columbia University in NY'.split())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRFClassifier invoked on Mon Aug 10 16:27:36 UTC 2020 with arguments:\n",
            "   -loadClassifier /content/english.muc.7class.distsim.crf.ser.gz -textFile /tmp/tmpnyrkd4pf -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions \"tokenizeNLs=false\" -encoding utf-8\n",
            "tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer\n",
            "Unknown property: |tokenizerFactory|\n",
            "tokenizerOptions=\"tokenizeNLs=false\"\n",
            "Unknown property: |tokenizerOptions|\n",
            "loadClassifier=/content/english.muc.7class.distsim.crf.ser.gz\n",
            "encoding=utf-8\n",
            "Unknown property: |encoding|\n",
            "textFile=/tmp/tmpnyrkd4pf\n",
            "outputFormat=slashTags\n",
            "Loading classifier from /content/english.muc.7class.distsim.crf.ser.gz ... Error deserializing /content/english.muc.7class.distsim.crf.ser.gz\n",
            "Exception in thread \"main\" java.lang.RuntimeException: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.ArrayList is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')\n",
            "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1380)\n",
            "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1331)\n",
            "\tat edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2315)\n",
            "Caused by: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.ArrayList is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')\n",
            "\tat edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2164)\n",
            "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1249)\n",
            "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1366)\n",
            "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1377)\n",
            "\t... 2 more\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8e4553d08d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baptiste Capdeville is studying at Columbia University in NY'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m--> 107\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Java command failed : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/content/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/content/english.muc.7class.distsim.crf.ser.gz', '-textFile', '/tmp/tmpnyrkd4pf', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf-8']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy28r3UfQn3U",
        "colab_type": "text"
      },
      "source": [
        "NER-tagging with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2FKB4OxLeBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp4L_Lc0Qwko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_0 = nlp(u'Donald Trump visited at the government headquarters in France today.')\n",
        "sent_1 = nlp(u'Emmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France and ex officio Co-Prince of Andorra since 14 May 2017.')\n",
        "sent_2 = nlp(u\"He studied philosophy at Paris Nanterre University, completed a Master's of Public Affairs at Sciences Po, and graduated from the École nationale d'administration (ÉNA) in 2004.\")\n",
        "sent_3 = nlp(u'He worked at the Inspectorate General of Finances, and later became an investment banker at Rothschild & Cie Banque.')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgxUevuRAMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f7b55343-13a9-4985-9a2f-62ac453c0df9"
      },
      "source": [
        "for token in sent_0:\n",
        "  print(token.text, token.ent_type_,token.pos_)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Donald PERSON PROPN\n",
            "Trump PERSON PROPN\n",
            "visited  VERB\n",
            "at  ADP\n",
            "the  DET\n",
            "government  NOUN\n",
            "headquarters  NOUN\n",
            "in  ADP\n",
            "France GPE PROPN\n",
            "today DATE NOUN\n",
            ".  PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuDQEamuRTJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "84daae7f-4f00-4679-8a89-9fd77f265971"
      },
      "source": [
        "for ent in sent_0.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Donald Trump PERSON\n",
            "France GPE\n",
            "today DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT_DEkXhRrXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "5933cb9f-2be0-43e8-d527-9912cf0cb517"
      },
      "source": [
        "#sentence1\n",
        "for token in sent_1:\n",
        "  print(token.text, token.ent_type_)\n",
        "\n",
        "for ent in sent_1.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emmanuel PERSON\n",
            "Jean PERSON\n",
            "- PERSON\n",
            "Michel PERSON\n",
            "Frédéric PERSON\n",
            "Macron PERSON\n",
            "is \n",
            "a \n",
            "French NORP\n",
            "politician \n",
            "serving \n",
            "as \n",
            "President \n",
            "of \n",
            "France GPE\n",
            "and \n",
            "ex \n",
            "officio \n",
            "Co PERSON\n",
            "- PERSON\n",
            "Prince PERSON\n",
            "of \n",
            "Andorra ORG\n",
            "since \n",
            "14 DATE\n",
            "May DATE\n",
            "2017 DATE\n",
            ". \n",
            "Emmanuel Jean-Michel Frédéric Macron PERSON\n",
            "French NORP\n",
            "France GPE\n",
            "Co-Prince PERSON\n",
            "Andorra ORG\n",
            "14 May 2017 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U5JhoJWSlU0",
        "colab_type": "text"
      },
      "source": [
        "# Training our own NER-taggers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWpFXax8R-er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plac\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "import warnings"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czaNms2oT8Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA = [\n",
        "              (\"who is Shaka Khan?\", {'entities': [(7,17,\"PERSON\")]}),\n",
        "              (\"I like London and Berlin.\",{'entities': [(7,13,\"LOC\"),(18,24,\"LOC\")]})\n",
        "]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2fbNDzVLIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1e1805d-9c45-4391-8459-f777b9267f3f"
      },
      "source": [
        "plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function plac_core.annotations.<locals>.annotate>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le_j7tXKVY9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(model = None, output_dir=None, n_iter=100):\n",
        "  \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "\n",
        "  if model is not None:\n",
        "    nlp = spacy.load(model)\n",
        "    print(\"Loaded model '%s'\" % model)\n",
        "  else:\n",
        "    nlp = spacy.blank('en')\n",
        "    print('created blank model')\n",
        "\n",
        "  # create the built-in pipeline components and add them to the pipeline\n",
        "  # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "\n",
        "  if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "  else:\n",
        "    ner = np.get_pipe(\"ner\")\n",
        "\n",
        "  #add the labels:\n",
        "  for _,annotations in TRAIN_DATA:\n",
        "    for ent in annotations.get('entities'):\n",
        "      ner.add_label(ent[2])\n",
        "  \n",
        "  #Get the other pipes and disable them before training:\n",
        "  pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "  other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "  #disable the other_pipe\n",
        "  with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
        "    # show warnings for misaligned entity spans once\n",
        "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
        "\n",
        "\n",
        "    if model is None:\n",
        "      nlp.begin_training()\n",
        "    for itr in range(n_iter):\n",
        "      random.shuffle(TRAIN_DATA)\n",
        "      losses={}\n",
        "      batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "      for batch in batches:\n",
        "        text,annotations = zip(*batch)\n",
        "        nlp.update(text, annotations, drop=0.5, losses=losses,)\n",
        "      print(\"losses\",losses)\n",
        "\n",
        "  # test the trained model\n",
        "  for text, _ in TRAIN_DATA:\n",
        "      doc = nlp(text)\n",
        "      print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "      print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "  \n",
        "  # save model to output directory\n",
        "  output_dir='/content/ner'\n",
        "  if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "      output_dir.mkdir()\n",
        "      nlp.to_disk(output_dir)\n",
        "      print(\"Saved model to\", output_dir)\n",
        "\n",
        "  # test the saved model\n",
        "  print(\"Loading from\", output_dir)\n",
        "  nlp2 = spacy.load(output_dir)\n",
        "  for text, _ in TRAIN_DATA:\n",
        "    doc = nlp2(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8svcPwcYYb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21f1b152-e466-4344-9bb5-991692ca2368"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created blank model\n",
            "losses {'ner': 9.899998903274536}\n",
            "losses {'ner': 9.633561372756958}\n",
            "losses {'ner': 9.289416193962097}\n",
            "losses {'ner': 9.011696577072144}\n",
            "losses {'ner': 8.505022048950195}\n",
            "losses {'ner': 8.523720264434814}\n",
            "losses {'ner': 7.158397316932678}\n",
            "losses {'ner': 6.730583012104034}\n",
            "losses {'ner': 7.2702717781066895}\n",
            "losses {'ner': 6.4985533356666565}\n",
            "losses {'ner': 5.960755467414856}\n",
            "losses {'ner': 5.4764240980148315}\n",
            "losses {'ner': 5.272995352745056}\n",
            "losses {'ner': 4.963782429695129}\n",
            "losses {'ner': 4.344571650028229}\n",
            "losses {'ner': 4.490230165421963}\n",
            "losses {'ner': 4.492963269352913}\n",
            "losses {'ner': 5.344881422817707}\n",
            "losses {'ner': 4.682546824216843}\n",
            "losses {'ner': 3.957813311368227}\n",
            "losses {'ner': 4.53189567476511}\n",
            "losses {'ner': 4.746835498139262}\n",
            "losses {'ner': 5.312202986795455}\n",
            "losses {'ner': 4.440175414085388}\n",
            "losses {'ner': 3.305656388401985}\n",
            "losses {'ner': 4.392327409237623}\n",
            "losses {'ner': 3.8977370988577604}\n",
            "losses {'ner': 3.834113746881485}\n",
            "losses {'ner': 3.6962173767387867}\n",
            "losses {'ner': 2.96488144248724}\n",
            "losses {'ner': 2.7720157094299793}\n",
            "losses {'ner': 2.185691963881254}\n",
            "losses {'ner': 2.4993351474404335}\n",
            "losses {'ner': 2.695954628288746}\n",
            "losses {'ner': 2.2740902733057737}\n",
            "losses {'ner': 2.3028083853423595}\n",
            "losses {'ner': 2.2033302187919617}\n",
            "losses {'ner': 1.9469671435654163}\n",
            "losses {'ner': 2.2984705418348312}\n",
            "losses {'ner': 4.532516898587346}\n",
            "losses {'ner': 1.914582408964634}\n",
            "losses {'ner': 2.676997670903802}\n",
            "losses {'ner': 2.567587248980999}\n",
            "losses {'ner': 1.0153732132166624}\n",
            "losses {'ner': 2.710034199524671}\n",
            "losses {'ner': 3.236407171934843}\n",
            "losses {'ner': 1.8833584421954583}\n",
            "losses {'ner': 2.8646702198311687}\n",
            "losses {'ner': 2.2207100223749876}\n",
            "losses {'ner': 3.2897060345858335}\n",
            "losses {'ner': 1.6790179204253945}\n",
            "losses {'ner': 1.8688670795727376}\n",
            "losses {'ner': 1.5226680630148621}\n",
            "losses {'ner': 1.69684734987095}\n",
            "losses {'ner': 2.744123948752531}\n",
            "losses {'ner': 1.338056057527865}\n",
            "losses {'ner': 1.2510107722864632}\n",
            "losses {'ner': 1.0199941160390154}\n",
            "losses {'ner': 0.8995625130264671}\n",
            "losses {'ner': 0.9424969043175224}\n",
            "losses {'ner': 0.9268889533159381}\n",
            "losses {'ner': 1.2498513816681225}\n",
            "losses {'ner': 0.934836907312274}\n",
            "losses {'ner': 0.3457425644155592}\n",
            "losses {'ner': 0.3963968826283235}\n",
            "losses {'ner': 0.29559312773199053}\n",
            "losses {'ner': 0.525839969253866}\n",
            "losses {'ner': 0.9777708915433827}\n",
            "losses {'ner': 0.03434183786890799}\n",
            "losses {'ner': 0.05185871323038782}\n",
            "losses {'ner': 0.011428380871636445}\n",
            "losses {'ner': 0.08772014501607828}\n",
            "losses {'ner': 0.5272628149602951}\n",
            "losses {'ner': 0.024428000794363136}\n",
            "losses {'ner': 0.008382834653957616}\n",
            "losses {'ner': 0.004370321756653084}\n",
            "losses {'ner': 9.423087951176967e-05}\n",
            "losses {'ner': 2.2472213627722226e-06}\n",
            "losses {'ner': 0.3070294258751658}\n",
            "losses {'ner': 0.009302259063510121}\n",
            "losses {'ner': 9.027244568686477e-05}\n",
            "losses {'ner': 0.3690002934910499}\n",
            "losses {'ner': 7.003630432222963e-05}\n",
            "losses {'ner': 0.009382981735704732}\n",
            "losses {'ner': 0.0037034516241156723}\n",
            "losses {'ner': 0.0002815034766892044}\n",
            "losses {'ner': 0.00013315677228096286}\n",
            "losses {'ner': 3.422041756244616e-06}\n",
            "losses {'ner': 0.0003490870071088392}\n",
            "losses {'ner': 5.0791171589634546e-05}\n",
            "losses {'ner': 0.0116737302287504}\n",
            "losses {'ner': 3.2003827006490265e-08}\n",
            "losses {'ner': 6.482016714069554e-05}\n",
            "losses {'ner': 2.0330295243953245e-06}\n",
            "losses {'ner': 1.4758508727796332e-05}\n",
            "losses {'ner': 1.3462090995695897e-05}\n",
            "losses {'ner': 0.004027648573579253}\n",
            "losses {'ner': 1.4639027357293832e-05}\n",
            "losses {'ner': 4.2260598158427864e-07}\n",
            "losses {'ner': 6.495837305632232e-05}\n",
            "Entities [('Shaka Khan', 'PERSON')]\n",
            "Tokens [('who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
            "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
            "Saved model to /content/ner\n",
            "Loading from /content/ner\n",
            "Entities [('Shaka Khan', 'PERSON')]\n",
            "Tokens [('who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
            "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRosthQTikm4",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/usage/training#section-ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dipks1EXYa1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}